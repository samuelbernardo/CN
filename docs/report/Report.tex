\documentclass{article}


\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage{pdfpages}
\usepackage{float}
\usepackage{amsmath}
%\usepackage{subfig} % entra em conflito com subcaption
\usepackage{geometry}
\usepackage{fancyhdr}
%\usepackage{newclude}
\usepackage{comment}
%\usepackage{units} %nicefrac
\usepackage{color} %white, black, red, green, blue, cyan, magenta, yellow.
\usepackage{multirow}
%\usepackage{ dsfont } % para o símbolo de números Reais
\usepackage[justification=centering]{caption}
\usepackage{subcaption} %para subfigures
%\usepackage{wrapfig}

\interfootnotelinepenalty=10000
\usepackage{hyperref}
\usepackage{listings}
\usepackage{setspace}
\usepackage{parskip}

%\bibliographystyle{plain}
\bibliographystyle{ieeetr}
\graphicspath{{./Imagens/}}

%FOR ALLOW STACKING ON THE RIGHT VERTICAL BAR
\makeatletter
\renewenvironment{subarray}[2][c]{%
  \if#1c\vcenter\else\vbox\fi\bgroup
  \Let@ \restore@math@cr \default@tag
  \baselineskip\fontdimen10 \thyristoriptfont\tw@
  \advance\baselineskip\fontdimen12 \thyristoriptfont\tw@
  \lineskip\thr@@\fontdimen8 \thyristoriptfont\thr@@
  \lineskiplimit\lineskip
  \ialign\bgroup\ifx c#2\hfil\fi
    $\m@th\thyristoriptstyle##$\hfil\crcr
}{%
  \crcr\egroup\egroup
}
\makeatother
\renewcommand{\substack}[2][c]{\subarray[#1]{c}#2\endsubarray}
%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\onehalfspacing %espaçamento entre linhas
%\setlength{\parindent}{0cm} %identação dos parágrafos
\parskip 4pt % espaçamento entre paragrafos


\include{Head}

\tableofcontents

\newpage

\section*{Introduction}
The first goal of the cloud computing project is to develop a mobile phone network data processing and search system. For achieving that goal we use a MapReduce application to process large logs of information retrieved from phone cell towers. The processed information is then stored on the database that will be latter accessed from a website.
 
The other goal for this project it so setup a load balancer and configure auto-scaling for the web servers. By configuring a proper load balancer and auto scaling mechanism, we should be able to grow the number of web servers when we detect more requests and drop web servers when there are few requests.

Within the next sections we will explain how we implemented the MapReduce application (detailing the most important factors of our solution), give a fast overview over the database organization and end up explaining the load balancing and auto scaling mechanisms.

The final sections will be dedicated to the evaluation and conclusions. Evaluation will be divided into: evaluating the MapReduce application and evaluating the load balancing and auto scaling.

\section{MapReduce Implementation}

The main goal for our MapReduce application is to efficiently process large sets of data and sent them to a remote database. In order to achieve it, we need to analyze what information is given as input and what information is expected for the output. The very big concern we had always in mind is that this application should do almost all the work and store an almost ready answer into the database. This is fundamental in order to achieve higher scalability (since the amount of processing inside the webserver and database will be very small).

The specification that we must cope with requests three types of queries: 

\begin{itemize}
\item get the sequence of cells where the phone was in a particular day;
\item get the set of phones present in one cell in a particular day and hour;
\item get the number of offline minutes for a particular phone and day.
\end{itemize}

Before going into implementation details, it is important to note the following: the first and the third queries can be processed in parallel only for different days and the second can be processed in parallel for different cells within one day.

Given the fact just presented, our implementation is very simple:

\begin{itemize}
\item \textbf{Map}: the map implementation takes all the lines and creates a different keys and values according to the event type. Events are grouped into three different sets of keys (that will be handled differently by the combiner and reducer) according to the event type;
\item \textbf{Combiner}: the combiner is used to process information for the second query (the one that can be processed in parallel for different cells);
\item \textbf{Reducer}: the reducer does the final aggregation of all the values. It also supports incremental logs (where by incremental logs we assume that are new logs from different celss). To provide the correct information, the reducer asks the database (whether it is an SQL or DynamoDB) to give the current values for the query. With this information, the reducer can produce the correct information that will then uploaded to the database. 
\end{itemize}

Another interesting feature that we used was secondary sorting. Secondary sorting enables us to perform value sorting and get all the values for the same key in order. This saves a lot of effort in the reducer since all the events are grouped and all the values inside each group are ordered.

Our final implementation is prepared to work in the RNL Hadoop installation and is able to produce output for three output storage systems: local, postgreSQL and Amazon DynamoDB. 

\section{Web Site}
We developed a web application to allow simple queries over the output data of the MapReduce application.
The web application is developed using php and javaScript. 
Those languages are the most used for simple web applications to allow the user to access and querying a 
database. In fact in our first prototype we store the results of the MapReduce application in a postreSQL database.
Anyway with php is also simple accessing to a BigTable on DynamoDB, as we did for the final application.
Three kind of queries are permitted: 

\subsection{User Interface}
With the website the user can access the data inserting the request specifications in the input fields.
To permit this there are three form blocks in the interface, one for each query. 
After the right insertion of the specifications in the input fields the request is sent to a php page due to 
accessing the data stored in a webserver.
The web interface is shown in figure ~\ref{webInterface} avaliable at: server url: \url{ec2-54-200-205-56.us-west-2.compute.amazonaws.com}, 
load balancer url: \url{ec2-54-201-119-18.us-west-2.compute.amazonaws.com/}.

\begin{figure}[H]
      \centering
      \includegraphics[scale = 0.40]{webInterface}
      \caption{Illustrative screenshot of index.html}
      \label{webInterface}
\end{figure}

\subsection{PHP: connection with data}
For retrieving the data stored in the web server (in a DynamoDB BigTable) we use a php page.
To allow stress test with software like 'siege' or 'jmeter', the querying specifications are passed to this
page with the GET method, directly written in the URL of the page.
Getting the attributes, the data are retrieved by the server. 
For the second and the third query the retrieved data are directly shown to the user, instead for the first 
query the data are stored in an hidden field for the last elaboration at the client web browser.
The listing of the php page is attached in the code ~\ref{php_code} and an example of the web application is
shown in figure ~\ref{phpPage}, available using the \url{http://ec2-54-200-205-56.us-west-2.compute.amazonaws.com/query.php?id=C1&date=2013%2F10%2F22&time=08&submit=query2}

\begin{figure}[H]
      \centering
      \includegraphics[scale = 0.40]{phpPage}
      \caption{Second query in query.php}
      \label{phpPage}
\end{figure}
\begingroup \fontsize{7pt}{7pt}\selectfont
\label{php_code}
\begin{verbatim}
query.php
<?php
//AWS connection
require '../aws/aws-autoloader.php';
use Aws\DynamoDb\DynamoDbClient;
$client=DynamoDbClient::factory(array(
  'key' => 'AKIAJSRJRTI5GRQNROMA',
  'secret' => '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',
  'region' => 'us-west-2'
));
//FORM GET STUFF
$submit = $_GET['submit'];
$date = $_GET['date'];
$id = $_GET['id'];
$dateId=$date."-".$id;
$time = "";
$sql = "";
//DynamoDb getItem
if ($submit!="query2"){
  if($submit == "query1"){
    $result = $client ->getItem(array(
        'ConsistentRead' => true,
        'TableName' => 'CN_logs',
        'Key' => array('date-id' => array('S'=>$dateId)),
        'AttributesToGet' => array('value')
        ));
  $output = $result['Item']['value']['SS'];
  }
  else{
    $result = $client ->getItem(array(
      'ConsistentRead' => true,
      'TableName' => 'CN_logs',
      'Key' => array('date-id' => array('S'=>$dateId)),
      'AttributesToGet' => array('number')
    ));
    $output = $result['Item']['number']['SS'];
  }
}
else{
$time = $_GET['time'];
$dateId=$dateId.":".$time;
$result = $client ->getItem(array(
      'ConsistentRead' => true,
      'TableName' => 'CN_logs',
      'Key' => array('date-id' => array('S'=>$dateId)),
      'AttributesToGet' => array('value')
      ));
$output = $result['Item']['value']['SS'];
}
echo "<div id='queryResult'>Results: <input type='hidden' id='queryNumber' value='$submit'/>";
echo "<div id='out'>";
if($submit!="query1"){
  echo "<table>";
  for ($i=0;$i<sizeof($output);$i++)
    echo "<tr><td>$output[$i]</td></tr>";
  echo "</table></div>";
}
else{
  echo "</div><input type='hidden' id='campo' value='";
  for ($i=0;$i<sizeof($output);$i++)
    echo $output[$i];
  echo"'/>";
}
echo "</div>";
?>
\end{verbatim}
\endgroup

\subsection{JavaScript: last elaboration}
As we saved the data in DynamoDB in a common table, the first query data needs one more elaboration step 
before be shown to the user. In fact we decided to elaborate the network logs by an unique MapReduce application
and store them in the most generic way in a common BigTable. To do this we decide to store int the fields of
the cells visited by a phoneId in a day also the time for having the sequence of the cells order by time.
The first query require only the ordered sequence of the cells, not the time. That sequence is stored in 
a hidden field in the web page and by a javaScript function the time is deleted from the sequence before 
been shown to the user. This function is called when the page sent by the server is loaded in the web 
browser of the user, for doing less work as possible in the server.
\begingroup \fontsize{7pt}{7pt}\selectfont
\begin{verbatim}
<body onload="init();">
Results:
<input id="queryNumber" type="hidden" value="query1">
<div id="out"> C2 C1</div>
<input id="campo" type="hidden" value="00:24:00;C2 14:12:15;C1">
\end{verbatim}
\begin{verbatim}
function init(){
if(document.getElementById("queryNumber").value=="query1")
document.getElementById('out').innerHTML=elaborateString(document.getElementById('campo').value);
}
function elaborateString(inputString){
  input=inputString.split(' ');
  output="";
  for (index=0;index<input.length;index++){
    item = input[index];
    item=item.split(";");
    output+=" "+item[item.length-1];
  }
  return output;
} 
\end{verbatim}
 \endgroup

\section{Load balancing and auto scaling}

Regarding the load balancing and auto scaling mechanisms we decided to use Amazon services since it would be impractical to use RNL to launch and terminate machines automatically. 

The big challenge here is to be able to grow and decrease the number of worker machines (in this case, web servers) according to the number of requests or load. In order to do this we created an auto scaling group with one load balancer, placed some scaling policies and setted up some alarms. 

After some experiments, we noted that the most significant factor is the CPU usage percentage. This is the only information available that we believe that truly explains the high load on the servers. As that being we placed two scaling policies and two alarms that are triggered. The fist policy adds one instance if the average CPU utilization (from the auto scaling group) is bigger than 35\% for more than one minute and another to remove a machine if the average CPU utilization is lower than 10\% for more than one minute. The time between scaling activities is 30 seconds.

Our scaling policies react very fast and try to add as soon as possible more machines. Although it can be a waste of resources for ephemeral waves of requests we decided to leave it like this for safety (this way we assure that we will loose less requests). For the health status we used 60 seconds intervall with an healthy threshold of 2 and an unhealthy threshold of 4 (to avoid loosing machines that are just too busy to respond). 

\section{Evaluation and Plots}
% micro instance, siege tests, httpd,php

\section{Conclusion}
\end{document}




